Context Window

The context window, also called context length or maximum sequence length, refers to the maximum number of tokens a language model can process in a single forward pass. It represents the model's "working memory" – the amount of text it can consider when generating responses. Context windows are measured in tokens, where one token roughly equals 0.75 words in English.

Early transformer models had context windows of 512-1024 tokens, limiting their ability to process long documents or maintain extended conversations. Modern LLMs have dramatically expanded this: GPT-3.5 supports 4K-16K tokens, GPT-4 offers 8K-128K variants, Claude supports up to 200K tokens, and research models push toward million-token contexts. Larger context windows enable processing entire books, codebases, or lengthy conversation histories without truncation. However, they come with computational costs – attention complexity scales quadratically with sequence length, though techniques like sparse attention and sliding windows mitigate this.

In conversational AI, context window management is critical. Systems must balance including relevant conversation history, retrieved knowledge (RAG), system prompts, and user input within token limits. Strategies include conversation summarization, sliding window approaches that keep recent exchanges, and selective retrieval of relevant past interactions. Running out of context mid-conversation forces truncation, potentially losing critical information. Understanding context window constraints shapes architecture decisions, from chunking strategies in RAG to conversation design in multi-turn dialogues.
